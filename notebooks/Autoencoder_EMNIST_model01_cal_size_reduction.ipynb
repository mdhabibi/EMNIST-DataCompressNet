{"cells":[{"cell_type":"markdown","metadata":{"id":"E-LZT7NvKYcF"},"source":["<h1 align=\"center\"><b>Efficient EMNIST Compression with Autoencoders</b></h1>\n","\n","<h3 align=\"center\">Computing the size of compressed original EMNIST dataset at the end of encoder</h3>\n","\n","---\n","---\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"Import necessary libraries\"></a>\n","## Import necessary libraries"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/mahdihabibi/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n","  warnings.warn(\n"]}],"source":["# Data Handling and Numerical Libraries\n","import random\n","import numpy as np\n","\n","# Keras - Deep Learning API\n","import keras                                # High-level neural networks API\n","from extra_keras_datasets import emnist     # EMNIST dataset of hand-written digits and lowercase+uppercase English alphabets\n","from keras import backend as K \n","from keras.layers import (                  # Neural network layers\n","    Conv2D, Conv2DTranspose, \n","    Input, Flatten, Dense, \n","    Reshape\n",")"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"Data Acquisition\"></a>\n","## Data Acquisition"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:root:Loading dataset = emnist\n","WARNING:root:Please cite the following paper when using or referencing this Extra Keras Dataset:\n","WARNING:root:Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). EMNIST: an extension of MNIST to handwritten letters. Retrieved from http://arxiv.org/abs/1702.05373\n"]}],"source":["# Data loading\n","(x_train, _), (x_test, _) = emnist.load_data(type='balanced')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Data Type : uint8\n","Data Shape: (112800, 28, 28)\n"]}],"source":["print(f\"Data Type : {x_train.dtype}\")\n","print(f\"Data Shape: {x_train.shape}\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of x_train with unit8 format in memory: 84.33837890625 MB\n"]}],"source":["x_train_size_bytes_uint8     = x_train.size * x_train.itemsize\n","x_train_size_megabytes_unit8 = x_train_size_bytes_uint8 / (1024 * 1024)\n","\n","print(f\"Size of x_train with unit8 format in memory: {x_train_size_megabytes_unit8} MB\")"]},{"cell_type":"markdown","metadata":{},"source":["**Note:** Data type: `uint8`, it means that the elements in the `x_train` array are of the type \"unsigned 8-bit integer\". In practical terms, a `uint8` data type can represent integers ranging from $0$ to $255$ (inclusive). This is a common format for image data where the intensity of each color channel (red, green, blue) in each pixel is represented as an integer from $0$ (no intensity) to $255$ (maximum intensity). So, if `x_train` is coming from an image dataset like EMNIST-Balanced, each element of `x_train` is an integer between $0$ and $255$ representing the grayscale intensity of a pixel in an image. Neural networks usually perform better with floating-point numbers, and work better with data in a normalized form, i.e., in the range of $0-1$. Converting the data type to 'float32' allows us to perform this normalization."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Data Type: float32\n"]}],"source":["# Normalize the images to [0, 1]\n","x_train = x_train.astype('float32') / 255.\n","x_test  = x_test .astype('float32') / 255.\n","\n","# Check again the type of elements in x_train after formatting\n","print(f\"Data Type: {x_train.dtype}\")"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of x_train with float32 format in memory: 337.353515625 MB\n"]}],"source":["x_train_size_bytes_float32     = x_train.size * x_train.itemsize\n","x_train_size_megabytes_float32 = x_train_size_bytes_float32 / (1024 * 1024)\n","\n","print(f\"Size of x_train with float32 format in memory: {x_train_size_megabytes_float32} MB\")"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The EMNIST-Balanced train set contains 112800 images, each with dimensions:\n","(width x height) = (28 x 28) pixels.\n","The EMNIST-Balanced test set also contains 18800 images with the same dimensions as the train set.\n"]}],"source":["# Retrieving the number of images and their dimensions from the train set\n","img_num_train, img_height, img_width = x_train.shape[:3]\n","\n","# Displaying the train set information\n","print(f\"The EMNIST-Balanced train set contains {img_num_train} images, each with dimensions:\"\n","      f\"\\n(width x height) = ({img_width} x {img_height}) pixels.\")\n","\n","# Retrieving the number of images from the test set\n","img_num_test  = x_test.shape[0]\n","\n","# Displaying the test set information\n","print(f\"The EMNIST-Balanced test set also contains {img_num_test} images with the same dimensions as the train set.\")"]},{"cell_type":"markdown","metadata":{},"source":["**Note**: In machine learning libraries like Keras, images need to be formatted in a specific shape (height, width, channels). The term \"channels\" refers to the number of color channels in the image. For grayscale images, there is only one channel. Therefore, we need to reshape our image data to fit this format."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dimensions of each image for the model: (img_height, img_width, num_channels) = (28, 28, 1).\n","Reshaped training data shape: (112800, 28, 28, 1)\n"]}],"source":["# Define the number of channels: 1 for grayscale images\n","num_channels = 1\n","\n","# Reshape the training and test datasets to include the channel dimension\n","x_train = x_train.reshape(img_num_train, img_height, img_width, num_channels)\n","x_test  = x_test .reshape(img_num_test , img_height, img_width, num_channels)\n","\n","# Define the input dimensions for the CNN\n","input_dimensions = (img_height, img_width, num_channels)\n","\n","# Display the reshaped dimensions\n","print(f\"Dimensions of each image for the model: (img_height, img_width, num_channels) = {input_dimensions}.\")\n","print(f\"Reshaped training data shape: {x_train.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"Model Architecture\"></a>\n","## Model Architecture\n","\n","\n","Autoencoders consist of two main parts:\n","\n","1. **Encoder:** This part of the network compresses the input into a latent-space representation. It encodes the input data as a compressed representation in a reduced dimension. The encoder layer is typically followed by several hidden layers that help the network learn complex patterns in the data."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Output shape of encoder: (None, 32)\n"]}],"source":["# Encoder\n","\n","# Input Layer: Defines the shape of the input data for the encoder.\n","encoder_input_layer = Input(shape=input_dimensions, name='encoder_input_layer')\n","\n","# Convolution Layers: Applies convolution operations to extract features from the input image.\n","encoder_layer = Conv2D(32, 3, padding='same', activation='relu')(encoder_input_layer)\n","encoder_layer = Conv2D(64, 3, padding='same', activation='relu', strides=(2, 2))(encoder_layer)\n","encoder_layer = Conv2D(64, 3, padding='same', activation='relu')(encoder_layer)\n","encoder_layer = Conv2D(64, 3, padding='same', activation='relu')(encoder_layer)\n","\n","# Flattening Layer: Converts the 3D output of convolution layers into a 1D tensor for dense layers.\n","encoder_layer = Flatten()(encoder_layer)\n","\n","# Dense Layer: A fully connected layer that combines extracted features and performs further learning.\n","encoder_layer = Dense(32, activation='relu')(encoder_layer)\n","\n","# Storing the output shape for use in the decoder\n","encoder_output_shape = K.int_shape(encoder_layer)\n","print(f\"Output shape of encoder: {encoder_output_shape}\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total size of compressed dataset: 13.76953125 MB\n"]}],"source":["# Shape of the encoder is (None, 32) and using float32 data type\n","compressed_size_per_image = 32 * 4  # 32 elements, each 4 bytes for float32\n","\n","# Total number of images in the dataset\n","num_images = 112800  \n","\n","# Total compressed size in bytes\n","total_compressed_size_bytes = num_images * compressed_size_per_image\n","\n","# Convert bytes to megabytes\n","total_compressed_size_megabytes = total_compressed_size_bytes / (1024 * 1024)\n","\n","print(f\"Total size of compressed dataset: {total_compressed_size_megabytes} MB\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
